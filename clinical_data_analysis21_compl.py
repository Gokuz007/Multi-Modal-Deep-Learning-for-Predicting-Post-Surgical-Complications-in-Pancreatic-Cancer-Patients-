# -*- coding: utf-8 -*-
"""Clinical Data Analysis21-COMPL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iPI0tvITJdjl3kRsYTy8mYu-GCG8QMIa
"""

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
import matplotlib.pyplot as plt
import shap
from torch.utils.data import DataLoader, Dataset, TensorDataset
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.preprocessing import LabelEncoder
from scipy.stats import chi2_contingency
from imblearn.over_sampling import SMOTE
from shap import LinearExplainer

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

"""**Data Loading**"""



df = pd.read_excel("/content/drive/My Drive/Thesis/PORSCH_complete.xlsx")

df.head()

"""**Data Preprocessing**"""

#filtering out relevant columns
columns_to_retain = [
    #'record_id',
    'gender_castor_x',
    'age_surgery_x',
    'datopn',
    'neoadjuvant_castor_10',
    'sandostatine',
    'typok',
    'drain_castor',
    'sof',
    'mof',
    'operative_bloodloss_compl',
    'pa_groups',
    'lengte',
    'gewicht',
    'gewverlies',
    'bilirubine',
    'hemoglobine',
    'albu',
    'origine',
    'histdiagnpost',
    'diameterpost',
    'radmarge',
    'differentiatie',
    'diffnet',
    'stadptpanc2018',
    'crp',
    'smra',
    'muscle_area',
    'vat_area',
    'sat_area',
    'smra_neo',

    'drain_aantal',
    'operative_time',
    'aantal_invasive_intervent',
    'ic_uopname',
    'adjuvant_treatment',
    'pd',
    'invasive_interv',
    'ic',
    'ic_later',
    'neoadjuvant_yn',
    'crpopf',
    'c_rgallek',
    'c_rchylus',
    'crdge',
    'crpph',
    'minimally_invasive_resection_yn',
    'invasief',
    'resecveneus',
    'resecart',
    'resecaanv',
    'drain_dpca',
    'octreo',
    'compl',
    'pneumonie',
    'wondinfectie',
    'transfusie',

    'neoadjuvant_chemo',
    #'implementphase',
    'typechemotherapy',
    #'minimally_invasive_resection_yn',
    'stadptpanc',
    'radicaliteit',
    #'major_complications'
]

df = df[columns_to_retain]

print("Columns retained in the DataFrame:")
print(df.columns.tolist())
print(df.shape)

#columns to be excluded based on missing values
columns_to_be_excluded = ['diffnet',
                          'smra_neo',
                          'stadptpanc2018',
                          'differentiatie',
                          'albu',
                          'crp',
                          'diameterpost',
                          'gewverlies',
                          'radmarge',
                         'neoadjuvant_chemo',
                         'typechemotherapy',
                         'stadptpanc']


df = df.drop(columns = columns_to_be_excluded)

df.shape

#Date Feature
#replace missing data with place holder
df['datopn'] = pd.to_datetime(df['datopn'],errors = 'coerce')

df['datopn'] = df['datopn'].fillna(pd.to_datetime('2018-12-11 00:00:00'))

df['dat_year'] = df['datopn'].dt.year
df['dat_month'] = df['datopn'].dt.month
df.drop('datopn',axis=1,inplace =True)

#replace the missing values withh medain for numerical features
numerical_feraures = [
    'age_surgery_x',
    'operative_bloodloss_compl',
    'lengte',
    'gewicht',
    'bilirubine',
    'hemoglobine',
    'smra',
    'muscle_area',
    'vat_area',
    'sat_area',

    'operative_time'
]

#Initialize imputer with median
imputer_num = SimpleImputer(strategy = 'median')

#transform the numerical features
df[numerical_feraures] = imputer_num.fit_transform(df[numerical_feraures])

print('Missing values in numerical column')
print(df[numerical_feraures].isnull().sum())

#replace the missing values withh medain for categorical features
#categ features
categorical_features = [
    'gender_castor_x',
    'neoadjuvant_castor_10',
    'sandostatine',
    'typok',
    'drain_castor',
    'sof',
    'mof',
    'origine',
    'histdiagnpost',
    'pa_groups',

    'drain_aantal',
    'aantal_invasive_intervent',
    'ic_uopname',
    'adjuvant_treatment',
    'pd',
    'invasive_interv',
    'ic',
    'ic_later',
    'neoadjuvant_yn',
    'crpopf',
    'c_rgallek',
    'c_rchylus',
    'crdge',
    'crpph',
    'minimally_invasive_resection_yn',
    'invasief',
    'resecveneus',
    'resecart',
    'resecaanv',
    'drain_dpca',
    'octreo',
    #'compl',
    'pneumonie',
    'wondinfectie',
    'transfusie',
    #'implementphase',
    #'minimally_invasive_resection_yn',
    'radicaliteit'
]

# Initialize the imputer with mode
imputer_cat = SimpleImputer(strategy='most_frequent')

df[categorical_features] = imputer_cat.fit_transform(df[categorical_features])

print('Missing values iin categorical column')
print(df[categorical_features].isnull().sum())

#numerical feature crorelation
numerical_feraures = [
    'age_surgery_x',
    'operative_bloodloss_compl',
    'lengte',
    'gewicht',
    'bilirubine',
    'hemoglobine',
    'smra',
    'muscle_area',
    'vat_area',
    'sat_area',
    'compl',
    'aantal_invasive_intervent',
    'operative_time'
]
numeric_df = df[numerical_feraures]
correlation_matrix = numeric_df.corr()
target_corr_threshold = 0.1
high_corr_features = correlation_matrix.index[abs(correlation_matrix["compl"]) > target_corr_threshold]
print(high_corr_features)

#chi-square test for categorical columns

categorical_cols = [
    'gender_castor_x',
    'neoadjuvant_castor_10',
    'sandostatine',
    'typok',
    'drain_castor',
    'sof',
    'mof',
    'origine',
    'histdiagnpost',
    'pa_groups',

    'drain_aantal',
    #'aantal_invasive_intervent',
    'ic_uopname',
    'adjuvant_treatment',
    'pd',
    'invasive_interv',
    'ic',
    'ic_later',
    'neoadjuvant_yn',
    'crpopf',
    'c_rgallek',
    'c_rchylus',
    'crdge',
    'crpph',
    'minimally_invasive_resection_yn',
    'invasief',
    'resecveneus',
    'resecart',
    'resecaanv',
    'drain_dpca',
    'octreo',
    'compl',
    'pneumonie',
    'wondinfectie',
    'transfusie',
   # 'implementphase',
    #'minimally_invasive_resection_yn',
    'radicaliteit'

]

label_encoder = LabelEncoder()

significant_features = []

for col in categorical_cols:
    contingency_table = pd.crosstab(df[col], df['compl'])
    chi2, p, _, _ = chi2_contingency(contingency_table)


    if p < 0.01:
        significant_features.append(col)

print("Significant categorical features related to 'compl':", significant_features)

df.shape
print(df.columns)

#df = df[['gender_castor_x','sandostatine','typok','sof','mof','operative_bloodloss_compl','muscle_area','gewicht','vat_area','bilirubine', 'age_surgery_x', 'origine','pa_groups','major_complications']]
#df = df[['gender_castor_x','sandostatine','typok','operative_bloodloss_compl','muscle_area','gewicht','vat_area','bilirubine', 'age_surgery_x', 'origine','pa_groups','major_complications']]
df = df[['gender_castor_x','sandostatine','typok',
         #'sof','mof',
         'operative_bloodloss_compl',
         'muscle_area','gewicht','vat_area',
         'bilirubine','hemoglobine', 'age_surgery_x',
         'origine','pa_groups','operative_time',
         #'aantal_invasive_intervent',
         'ic_uopname', 'adjuvant_treatment', 'pd',
         #'invasive_interv',
         #'ic', 'ic_later',
         #'crpopf', 'c_rgallek', 'c_rchylus', 'crdge', 'crpph',
         'invasief',
         'drain_dpca',
         'octreo',
         'compl',
         #'pneumonie', 'wondinfectie', 'transfusie',

        ]]

"""Encoding Categorical Variables"""

#enocding binary features
#binary_features = ['gender_castor_x', 'drain_castor', 'sof', 'mof']
#binary_features = ['gender_castor_x', 'sof', 'mof']
#binary_features = ['gender_castor_x', 'sof', 'mof',
#                   'ic_uopname','adjuvant_treatment', 'pd', 'invasive_interv', 'ic', 'ic_later', 'crpopf', 'c_rgallek', 'c_rchylus', 'crdge', 'crpph','drain_dpca', 'compl', 'pneumonie', 'wondinfectie', 'transfusie']
binary_features = ['gender_castor_x',
                   #'sof', 'mof',
                   'ic_uopname','adjuvant_treatment', 'pd',
                   #'invasive_interv',
                   #'ic', 'ic_later',
                   #'crpopf', 'c_rgallek', 'c_rchylus', 'crdge', 'crpph',
                   'drain_dpca',
                   'octreo'
                   #'compl', 'pneumonie', 'wondinfectie', 'transfusie'
                  # 'radicaliteit'
                  ]

df[binary_features] = df[binary_features].astype(int)

df[binary_features].dtypes

#multi-class categorical features to be one-hot encoded
multi_class_features = [
    #'neoadjuvant_castor_10',
    'sandostatine',
    'typok',
    'pa_groups',
    'origine',
    #'histdiagnpost'
    'invasief'
]

df = pd.get_dummies(df, columns = multi_class_features, drop_first=False)

df.head(10)



"""Feature Scaling"""

numerical_features_scaled = [
    'age_surgery_x',
    'operative_bloodloss_compl',
    #'lengte',
    'gewicht',
    'bilirubine',
    'hemoglobine',
    #'smra',
    'muscle_area',
    'vat_area',
    #'sat_area',
    #'dat_year',
    #'dat_month'
    'operative_time',
    #'aantal_invasive_intervent'
]

#intitalize scaler
scaler = StandardScaler()


df[numerical_features_scaled] = scaler.fit_transform(df[numerical_features_scaled])

df.head(5)

target_counts = df['compl'].value_counts()

print("Class Distribution:" ,target_counts)

df['compl'].isnull().sum()

df = df.dropna(subset='compl')
len(df)

"""**Train-Test Split**"""

#params
batch_size = 8
learning_rate = 1e-4
num_epochs = 150
weight_decay = 1e-4
save_location = '/content/drive/My Drive/Thesis/best_model2compl.pth'

#class distribution
target_counts = df['compl'].value_counts()
target_percent = (target_counts/len(df))*100
print('Class Distribution:',target_counts)
print('Class Dist Percentage:',target_percent)

df.shape

X = df.drop('compl', axis =1)
y = df['compl']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42, stratify=y)

print("Training set:",y_train.value_counts())
print('Testing set',y_test.value_counts())

X_train.columns

feature_columns = X_train.columns.tolist()

X_train = X_train.astype('float64')
X_test = X_test.astype('float64')

print("Data types of X_train:")
print(X_train.dtypes)

# Apply SMOTE to the training data
#smote = SMOTE(random_state=42)
#X_train_res, y_train_res = smote.fit_resample(X_train, y_train)
#
#print("\nAfter SMOTE, training set class distribution:")
#print(y_train_res.value_counts())

#Convert Pandas DFs to NumPy arrays
X_train = X_train.values
X_test = X_test.values
y_train = y_train.values
y_test = y_test.values

#convert to tensors since we load tensors
X_train = torch.tensor(X_train.astype(float), dtype=torch.float32)
X_test = torch.tensor(X_test.astype(float), dtype=torch.float32)
y_train = torch.tensor(y_train.astype(float), dtype=torch.float32).unsqueeze(1)
y_test = torch.tensor(y_test.astype(float), dtype=torch.float32).unsqueeze(1)

#create tensordatasets
train_dataset = TensorDataset(X_train, y_train)
test_dataset = TensorDataset(X_test, y_test)

#create dataloaders
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,drop_last=True )
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

"""**FNN model**"""

class Comp_FNN(nn.Module):
  def __init__(self, input_size):
    super(Comp_FNN,self).__init__()

    # Fully connected layers
    self.fc1 = nn.Linear(input_size,64)
    self.fc2 = nn.Linear(64,32)
    self.fc3 = nn.Linear(32,16)
    self.fc4 = nn.Linear(16,1)

    self.dropout = nn.Dropout(0.3)

    self.batch_norm1 = nn.BatchNorm1d(64)
    self.batch_norm2 = nn.BatchNorm1d(32)
    self.batch_norm3 = nn.BatchNorm1d(16)

  def forward(self, x):
    x1 = F.relu(self.batch_norm1(self.dropout(self.fc1(x))))
    x2 = F.relu(self.batch_norm2(self.dropout(self.fc2(x1))))
    x3 = F.relu(self.batch_norm3(self.dropout(self.fc3(x2))))
    x4 = torch.sigmoid(self.fc4(x3))

    return x4

# Get the number of input features
input_size = X_train.shape[1]
# Instantiate the model
model = Comp_FNN(input_size)

print(model)

"""**Loss Function and Optimizer**"""



#Loss function - binary cross entropy
loss_function = nn.BCELoss()
# Define hyperparameters for Focal Loss
alpha = 0.4  # Adjust based on class imbalance
gamma = 1.5   # Common starting point

# Instantiate Focal Loss
#loss_function = FocalLoss(alpha=alpha, gamma=gamma, reduction='mean')

#Optimizer - Adam
optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate, weight_decay= weight_decay)

# Move the model to the device
model.to(device)

"""**Training Loop**"""

#initialize variables for ealy stopping
patience = 15
best_auc = 0
counter = 0

# To store training history
logs = {
    'train_loss': [],
    'train_auc': [],
    'val_loss': [],
    'val_auc': [],
    'val_accuracy': [],
    'val_precision': [],
    'val_recall': [],
    'val_f1': []
}

# FOR SAVING logs for the best epoch
best_logs = {
    'epoch':None,
    'train_loss': None,
    'train_auc': None,
    'val_loss': None,
    'val_auc': None,
    'val_accuracy': None,
    'val_precision': None,
    'val_recall': None,
    'val_f1': None
}

for epoch in range(num_epochs):
    # Training Phase
    model.train()
    train_loss = 0.0
    preds = []
    true_labels = []
    for inputs, labels in train_loader:
        inputs, labels = inputs.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = loss_function(outputs, labels)

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

        # Accumulate loss
        # Multiply the average loss by batch size to get total loss for the batch
        # Train loss is the total loss over all batches in the epoch
        train_loss += loss.item() * inputs.size(0)

        preds.append(outputs.detach().cpu().numpy())
        true_labels.append(labels.detach().cpu().numpy())

    # Calculate average loss per sample for the entire training epoch
    epoch_train_loss = train_loss / len(train_loader.dataset)
    preds = np.vstack(preds)
    true_labels = np.vstack(true_labels)
    epoch_train_auc = roc_auc_score(true_labels.ravel(), preds.ravel())

    logs['train_loss'].append(epoch_train_loss)
    logs['train_auc'].append(epoch_train_auc)

    # Validation Phase
    model.eval()
    val_loss = 0.0
    val_preds = []
    val_true_labels = []

    with torch.no_grad():
        for inputs, labels in test_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            loss = loss_function(outputs, labels)

            val_loss += loss.item() * inputs.size(0)

            val_preds.append(outputs.detach().cpu().numpy())
            val_true_labels.append(labels.detach().cpu().numpy())

    epoch_val_loss = val_loss / len(test_loader.dataset)
    val_preds = np.vstack(val_preds)
    val_true_labels = np.vstack(val_true_labels)
    epoch_val_auc = roc_auc_score(val_true_labels.ravel(), val_preds.ravel())

    # Convert probabilities to binary predictions
    y_pred_bin = (val_preds.ravel() >= 0.5).astype(int)
    y_true_flat = val_true_labels.ravel()

    # Calculate metrics
    epoch_val_accuracy = accuracy_score(y_true_flat, y_pred_bin)
    epoch_val_precision = precision_score(y_true_flat, y_pred_bin, zero_division=0)
    epoch_val_recall = recall_score(y_true_flat, y_pred_bin, zero_division=0)
    epoch_val_f1 = f1_score(y_true_flat, y_pred_bin, zero_division=0)

    logs['val_loss'].append(epoch_val_loss)
    logs['val_auc'].append(epoch_val_auc)
    logs['val_accuracy'].append(epoch_val_accuracy)
    logs['val_precision'].append(epoch_val_precision)
    logs['val_recall'].append(epoch_val_recall)
    logs['val_f1'].append(epoch_val_f1)

    print(f"Epoch {epoch+1}/{num_epochs} - "
          f"Train Loss: {epoch_train_loss:.4f} - Train AUC: {epoch_train_auc:.4f} -  "
          f"Val Loss: {epoch_val_loss:.4f} - Val AUC: {epoch_val_auc:.4f} - "
          f"Val Acc: {epoch_val_accuracy:.4f} - Val Prec: {epoch_val_precision:.4f} - "
          f"Val Recall: {epoch_val_recall:.4f} - Val F1: {epoch_val_f1:.4f}")

    #early stopping
    if epoch_val_auc > best_auc:
      best_auc = epoch_val_auc
      best_logs['epoch'] = epoch + 1
      best_logs['val_loss'] = epoch_val_loss
      best_logs['val_auc'] = epoch_val_auc
      best_logs['val_accuracy'] = epoch_val_accuracy
      best_logs['val_precision'] = epoch_val_precision
      best_logs['val_recall'] = epoch_val_recall
      best_logs['val_f1'] = epoch_val_f1

      # Save the best model
      torch.save(model.state_dict(), save_location)
      print(f"** Best model saved at epoch {epoch+1} with Val AUC: {epoch_val_auc:.4f} **")
      counter = 0
    else:
      counter += 1

      if counter >= patience:
        print('Early Stopping')
        break

print("\n===== Best Epoch Metrics =====")
print("\nThe model with following metrics was saved")
print(f"Epoch: {best_logs['epoch']}")
print(f"Validation Loss: {best_logs['val_loss']:.4f}")
print(f"Validation AUC: {best_logs['val_auc']:.4f}")
print(f"Validation Accuracy: {best_logs['val_accuracy']:.4f}")
print(f"Validation Precision: {best_logs['val_precision']:.4f}")
print(f"Validation Recall: {best_logs['val_recall']:.4f}")
print(f"Validation F1-Score: {best_logs['val_f1']:.4f}")

# Plot Loss
plt.figure(figsize=(12,5))
plt.subplot(1,2,1)
plt.plot(logs['train_loss'], label='Training Loss')
plt.plot(logs['val_loss'], label='Validation Loss')
plt.title('Loss Over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Binary Crossentropy Loss')
plt.legend()

# Plot AUC
plt.subplot(1,2,2)
plt.plot(logs['train_auc'], label='Training AUC')
plt.plot(logs['val_auc'], label='Validation AUC')
plt.title('AUC Over Epochs')
plt.xlabel('Epoch')
plt.ylabel('AUC')
plt.legend()

plt.tight_layout()
plt.show()

plt.figure(figsize=(20,10))

# Subplot 1: Training and Validation Loss
plt.subplot(2,2,1)
plt.plot(logs['train_loss'], label='Training Loss', color='blue')
plt.plot(logs['val_loss'], label='Validation Loss', color='orange')
plt.title('Loss Over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Binary Crossentropy Loss')
plt.legend()
plt.grid(True)

# Subplot 2: Training and Validation AUC
plt.subplot(2,2,2)
plt.plot(logs['train_auc'], label='Training AUC', color='green')
plt.plot(logs['val_auc'], label='Validation AUC', color='purple')
plt.title('AUC Over Epochs')
plt.xlabel('Epoch')
plt.ylabel('AUC')
plt.legend()
plt.grid(True)

# Subplot 3: Validation Accuracy
plt.subplot(2,2,3)
plt.plot(logs['val_accuracy'], label='Validation Accuracy', color='red')
plt.title('Validation Accuracy Over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)

# Subplot 4: Validation F1-Score
plt.subplot(2,2,4)
plt.plot(logs['val_f1'], label='Validation F1-Score', color='purple')
plt.title('Validation F1-Score Over Epochs')
plt.xlabel('Epoch')
plt.ylabel('F1 Score')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()

counter





"""Explainability - SHAP"""

shap.initjs()

model.load_state_dict(torch.load(save_location, map_location=torch.device('cpu')))
model.to(torch.device('cpu'))
model.eval()



# 3. Prepare data for SHAP
X_test_np = X_test.cpu().numpy()
y_test_np = y_test.cpu().numpy()
feature_names = X.columns.tolist()

# 4. Define the prediction function
def model_predict(data):
    model.eval()
    with torch.no_grad():
        inputs = torch.tensor(data, dtype=torch.float32)
        outputs = model(inputs)
        probabilities = outputs.cpu().numpy().flatten()
    return probabilities

# 5. Initialize KernelExplainer with link="identity"
background_size = 100
background_indices = np.random.choice(X_train.shape[0], background_size, replace=False)
background = X_train[background_indices].cpu().numpy()

explainer = shap.KernelExplainer(model_predict, background, link="identity")

# 6. Select samples for explanation
explanation_size = 50
explanation_indices = np.random.choice(X_test_np.shape[0], explanation_size, replace=False)
explanation_samples = X_test_np[explanation_indices]

# 7. Compute SHAP values
shap_values = explainer.shap_values(explanation_samples, nsamples=100)

# 8. Inspect shap_values structure
print("Type of shap_values:", type(shap_values))
print("Shape of shap_values:", shap_values.shape)

# 9. Convert to DataFrame
samples_df = pd.DataFrame(explanation_samples, columns=feature_names)









# 10. Summary Bar Plot
plt.figure(figsize=(12, 8))
shap.summary_plot(shap_values, samples_df, plot_type="bar", feature_names=feature_names, show=False)
plt.title("SHAP Summary Bar Plot - Features Ordered by Importance")
plt.tight_layout()
plt.show()

# 11. Summary Dot Plot
plt.figure(figsize=(12, 8))
shap.summary_plot(shap_values, samples_df, feature_names=feature_names, show=False)
plt.title("SHAP Summary Dot Plot - Features Ordered by Importance")
plt.tight_layout()
plt.show()

# Create Waterfall plot for the first observation
first_shap_values = shap_values[0]  # SHAP values for the first sample

shap_explanation = shap.Explanation(
    values=first_shap_values,
    base_values=explainer.expected_value,
    data=samples_df.iloc[0],  # Pass as a pandas Series
    feature_names=feature_columns
)

shap.waterfall_plot(shap_explanation)
plt.show()



# 13. Force Plot for the first observation
shap.initjs()
shap.force_plot(
    explainer.expected_value,
    shap_values[0],
    samples_df.iloc[0],
    matplotlib=True
)



# 13. Individual Force Plot for the first sample
shap.initjs()
shap.force_plot(
    explainer.expected_value,
    shap_values_single[0],
    samples_df.iloc[0],
    matplotlib=True
)
plt.show()

from sklearn.metrics import confusion_matrix
import seaborn as sns

cm = confusion_matrix(y_test_np, y_pred_bin)
plt.figure(figsize=(8,6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['No Complications', 'Complications'], yticklabels=['No Complications', 'Complications'])
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.title('Confusion Matrix')
plt.show()